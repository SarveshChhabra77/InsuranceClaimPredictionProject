{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "217cc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6affa87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_sex</th>\n",
       "      <th>insured_education_level</th>\n",
       "      <th>insured_occupation</th>\n",
       "      <th>insured_hobbies</th>\n",
       "      <th>insured_relationship</th>\n",
       "      <th>capital-gains</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>collision_type</th>\n",
       "      <th>incident_severity</th>\n",
       "      <th>authorities_contacted</th>\n",
       "      <th>incident_state</th>\n",
       "      <th>incident_city</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>number_of_vehicles_involved</th>\n",
       "      <th>property_damage</th>\n",
       "      <th>bodily_injuries</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "      <th>policy_bind_year</th>\n",
       "      <th>policy_bind_month</th>\n",
       "      <th>policy_bind_day</th>\n",
       "      <th>incident_date_year</th>\n",
       "      <th>incident_date_month</th>\n",
       "      <th>incident_date_day</th>\n",
       "      <th>csl_per_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>OH</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>MD</td>\n",
       "      <td>craft-repair</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>husband</td>\n",
       "      <td>53300</td>\n",
       "      <td>0</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Side Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>SC</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>71610</td>\n",
       "      <td>Saab</td>\n",
       "      <td>92x</td>\n",
       "      <td>2004</td>\n",
       "      <td>Y</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>IN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>MALE</td>\n",
       "      <td>MD</td>\n",
       "      <td>machine-op-inspct</td>\n",
       "      <td>reading</td>\n",
       "      <td>other-relative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Vehicle Theft</td>\n",
       "      <td>No Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>VA</td>\n",
       "      <td>Riverwood</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "      <td>5070</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>Y</td>\n",
       "      <td>2006</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>OH</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>PhD</td>\n",
       "      <td>sales</td>\n",
       "      <td>board-games</td>\n",
       "      <td>own-child</td>\n",
       "      <td>35100</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-vehicle Collision</td>\n",
       "      <td>Rear Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>NY</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>34650</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2007</td>\n",
       "      <td>N</td>\n",
       "      <td>2000</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>IL</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>PhD</td>\n",
       "      <td>armed-forces</td>\n",
       "      <td>board-games</td>\n",
       "      <td>unmarried</td>\n",
       "      <td>48900</td>\n",
       "      <td>-62400</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Front Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>OH</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>63400</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Tahoe</td>\n",
       "      <td>2014</td>\n",
       "      <td>Y</td>\n",
       "      <td>1990</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>IL</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>sales</td>\n",
       "      <td>board-games</td>\n",
       "      <td>unmarried</td>\n",
       "      <td>66000</td>\n",
       "      <td>-46000</td>\n",
       "      <td>Vehicle Theft</td>\n",
       "      <td>No Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>NY</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>6500</td>\n",
       "      <td>Accura</td>\n",
       "      <td>RSX</td>\n",
       "      <td>2009</td>\n",
       "      <td>N</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age policy_state  ...  incident_date_day  csl_per_person\n",
       "0   48           OH  ...                 25             250\n",
       "1   42           IN  ...                 21             250\n",
       "2   29           OH  ...                 22             100\n",
       "3   41           IL  ...                 10             250\n",
       "4   44           IL  ...                 17             500\n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\Insurance Claim Prediction Project\\notebook\\data\\processed\\processed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75fb892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df.drop('fraud_reported',axis=1),df['fraud_reported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b82954e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.map({'Y': 1,'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed4cde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b2864d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing preprocessor and handling imbalance data\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "preprocessor_file_path=os.path.join(os.getcwd(),'preprocessor.pkl')\n",
    "\n",
    "with open(preprocessor_file_path,'rb') as file_obj:\n",
    "    preprocessor=pickle.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe904bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4641271 ,  1.3913488 , -0.53921609, ...,  0.19491767,\n",
       "         0.16570152,  0.22524942],\n",
       "       [-0.70026181, -0.23053989,  0.74079248, ...,  0.22826856,\n",
       "         0.13586366,  0.16332441],\n",
       "       [ 1.26396322, -0.23053989, -1.25881778, ...,  0.22826856,\n",
       "         0.22088077,  0.22524942],\n",
       "       ...,\n",
       "       [-0.37289097, -0.23053989, -1.37944569, ...,  0.19491767,\n",
       "         0.34754026,  0.24917852],\n",
       "       [-0.26376736, -0.23053989, -0.5391713 , ...,  0.26469871,\n",
       "         0.16570152,  0.22524942],\n",
       "       [ 1.1548396 , -0.23053989,  0.08058649, ...,  0.22826856,\n",
       "         0.16570152,  0.22524942]], shape=(300, 45))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## performing  standardization \n",
    "X_train = preprocessor.fit_transform(X_train,y_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c7e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(random_state=42)\n",
    "X_train,y_train=smote.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa103aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=False),\n",
    "    \"Naive Bayes (Gaussian)\": GaussianNB()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1d322",
   "metadata": {},
   "source": [
    "For fraud detection, recall is usually the most critical, because missing a fraudulent claim costs money, while a few false positives (extra checks) are manageable. So we focus mainly on recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "370a70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,roc_auc_score,precision_score,accuracy_score,f1_score\n",
    "def evaluate_model(y_true,y_pred):\n",
    "    model_recall_score = recall_score(y_true,y_pred)\n",
    "    model_roc_auc_score = roc_auc_score(y_true,y_pred)\n",
    "    model_precision_score = precision_score(y_true,y_pred)\n",
    "    model_accuracy_score = accuracy_score(y_true,y_pred)\n",
    "    model_f1_score = f1_score(y_true,y_pred)\n",
    "    return model_recall_score,model_roc_auc_score,model_precision_score,model_accuracy_score,model_f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00d9729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.9174\n",
      "- Roc Auc Score: 0.8884\n",
      "- Precision Score: 0.8670\n",
      "- Accuracy Score: 0.8884\n",
      "- Score: 0.8915\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.7500\n",
      "- Roc Auc Score: 0.7932\n",
      "- Precision Score: 0.6250\n",
      "- Accuracy Score: 0.8133\n",
      "- F1 Score: 0.6818\n",
      "===================================\n",
      "\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.9850\n",
      "- Roc Auc Score: 0.7702\n",
      "- Precision Score: 0.6890\n",
      "- Accuracy Score: 0.7702\n",
      "- Score: 0.8108\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.7875\n",
      "- Roc Auc Score: 0.6256\n",
      "- Precision Score: 0.3481\n",
      "- Accuracy Score: 0.5500\n",
      "- F1 Score: 0.4828\n",
      "===================================\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Model performance for Training set\n",
      "- Recall Score: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "- Precision Score: 1.0000\n",
      "- Accuracy Score: 1.0000\n",
      "- Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.4625\n",
      "- Roc Auc Score: 0.6403\n",
      "- Precision Score: 0.4805\n",
      "- Accuracy Score: 0.7233\n",
      "- F1 Score: 0.4713\n",
      "===================================\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Model performance for Training set\n",
      "- Recall Score: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "- Precision Score: 1.0000\n",
      "- Accuracy Score: 1.0000\n",
      "- Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.2375\n",
      "- Roc Auc Score: 0.5687\n",
      "- Precision Score: 0.4634\n",
      "- Accuracy Score: 0.7233\n",
      "- F1 Score: 0.3140\n",
      "===================================\n",
      "\n",
      "\n",
      "Extra Trees\n",
      "Model performance for Training set\n",
      "- Recall Score: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "- Precision Score: 1.0000\n",
      "- Accuracy Score: 1.0000\n",
      "- Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.3000\n",
      "- Roc Auc Score: 0.5955\n",
      "- Precision Score: 0.5000\n",
      "- Accuracy Score: 0.7333\n",
      "- F1 Score: 0.3750\n",
      "===================================\n",
      "\n",
      "\n",
      "AdaBoost\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.9268\n",
      "- Roc Auc Score: 0.9212\n",
      "- Precision Score: 0.9165\n",
      "- Accuracy Score: 0.9212\n",
      "- Score: 0.9216\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.4375\n",
      "- Roc Auc Score: 0.6460\n",
      "- Precision Score: 0.5224\n",
      "- Accuracy Score: 0.7433\n",
      "- F1 Score: 0.4762\n",
      "===================================\n",
      "\n",
      "\n",
      "Gradient Boosting\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.9775\n",
      "- Roc Auc Score: 0.9794\n",
      "- Precision Score: 0.9812\n",
      "- Accuracy Score: 0.9794\n",
      "- Score: 0.9793\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.4875\n",
      "- Roc Auc Score: 0.6756\n",
      "- Precision Score: 0.5652\n",
      "- Accuracy Score: 0.7633\n",
      "- F1 Score: 0.5235\n",
      "===================================\n",
      "\n",
      "\n",
      "XGBoost\n",
      "Model performance for Training set\n",
      "- Recall Score: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "- Precision Score: 1.0000\n",
      "- Accuracy Score: 1.0000\n",
      "- Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.5000\n",
      "- Roc Auc Score: 0.6864\n",
      "- Precision Score: 0.5882\n",
      "- Accuracy Score: 0.7733\n",
      "- F1 Score: 0.5405\n",
      "===================================\n",
      "\n",
      "\n",
      "CatBoost\n",
      "Model performance for Training set\n",
      "- Recall Score: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "- Precision Score: 1.0000\n",
      "- Accuracy Score: 1.0000\n",
      "- Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.4375\n",
      "- Roc Auc Score: 0.6528\n",
      "- Precision Score: 0.5469\n",
      "- Accuracy Score: 0.7533\n",
      "- F1 Score: 0.4861\n",
      "===================================\n",
      "\n",
      "\n",
      "Naive Bayes (Gaussian)\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.9306\n",
      "- Roc Auc Score: 0.7186\n",
      "- Precision Score: 0.6535\n",
      "- Accuracy Score: 0.7186\n",
      "- Score: 0.7678\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "Model performance for Training set\n",
      "- Recall Score: 0.8750\n",
      "- Roc Auc Score: 0.6943\n",
      "- Precision Score: 0.3955\n",
      "- Accuracy Score: 0.6100\n",
      "- F1 Score: 0.5447\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(list(classification_models))):\n",
    "    model = list(classification_models.values())[i]\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    y_train_pred=model.predict(X_train)\n",
    "    y_test_pred=model.predict(X_test)\n",
    "    \n",
    "    y_train_recall_score,y_train_roc_auc_score,y_train_precision_score,y_train_accuracy_score,y_train_f1_score=(evaluate_model(y_train,y_train_pred))\n",
    "    y_test_recall_score,y_test_roc_auc_score,y_test_precision_score,y_test_accuracy_score,y_test_f1_score=(evaluate_model(y_test,y_test_pred))\n",
    "    \n",
    "    print(list(classification_models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Recall Score: {:.4f}\".format(y_train_recall_score))\n",
    "    print(\"- Roc Auc Score: {:.4f}\".format(y_train_roc_auc_score))\n",
    "    print(\"- Precision Score: {:.4f}\".format(y_train_precision_score))\n",
    "    print(\"- Accuracy Score: {:.4f}\".format(y_train_accuracy_score))\n",
    "    print(\"- Score: {:.4f}\".format(y_train_f1_score))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Test set')\n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Recall Score: {:.4f}\".format(y_test_recall_score))\n",
    "    print(\"- Roc Auc Score: {:.4f}\".format(y_test_roc_auc_score))\n",
    "    print(\"- Precision Score: {:.4f}\".format(y_test_precision_score))\n",
    "    print(\"- Accuracy Score: {:.4f}\".format(y_test_accuracy_score))\n",
    "    print(\"- F1 Score: {:.4f}\".format(y_test_f1_score))\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61a34ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "        \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\", \"saga\", \"newton-cg\"],\n",
    "        \"max_iter\": [100, 200, 500],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \n",
    "    \"K-Nearest Neighbors\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "        \"p\": [1, 2]\n",
    "    },\n",
    "    \n",
    "    \"Decision Tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \n",
    "    \"Extra Trees\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \n",
    "    \"AdaBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200, 300],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.5, 1],\n",
    "        \"algorithm\": [\"SAMME\", \"SAMME.R\"]\n",
    "    },\n",
    "    \n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7, 10],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"gamma\": [0, 0.1, 0.3, 0.5],\n",
    "        \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
    "        \"reg_lambda\": [1, 1.5, 2]\n",
    "    },\n",
    "    \n",
    "    \"CatBoost\": {\n",
    "        \"depth\": [4, 6, 8, 10, 12],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "        \"iterations\": [200, 500, 800, 1000],\n",
    "        \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "        \"bagging_temperature\": [0, 0.5, 1, 2, 5],\n",
    "        \"random_strength\": [0, 1, 2, 5, 10],\n",
    "        \"rsm\": [0.6, 0.8, 1.0],\n",
    "        \"grow_policy\": [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"],\n",
    "        \"min_data_in_leaf\": [1, 5, 10, 20, 50],\n",
    "        \"max_bin\": [128, 254, 512]\n",
    "    },\n",
    "    \n",
    "    \"Naive Bayes (Gaussian)\": {\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6a10f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "70 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 72, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "16 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.90437313        nan        nan\n",
      "        nan 0.89874802        nan        nan        nan        nan\n",
      " 0.90063481        nan        nan        nan 0.73919944 0.90437313\n",
      " 0.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for Logistic Regression: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 500, 'class_weight': 'balanced', 'C': 1}\n",
      "Best CV R²: 0.9043731264327279\n",
      "--------------------------------------------------\n",
      "Tuning K-Nearest Neighbors...\n",
      "Best Params for K-Nearest Neighbors: {'weights': 'distance', 'p': 1, 'n_neighbors': 9, 'metric': 'euclidean'}\n",
      "Best CV R²: 0.9849761946746606\n",
      "--------------------------------------------------\n",
      "Tuning Decision Tree...\n",
      "Best Params for Decision Tree: {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 5, 'criterion': 'gini', 'class_weight': 'balanced'}\n",
      "Best CV R²: 0.9119202962440486\n",
      "--------------------------------------------------\n",
      "Tuning Random Forest...\n",
      "Best Params for Random Forest: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 30, 'criterion': 'entropy', 'class_weight': 'balanced', 'bootstrap': True}\n",
      "Best CV R²: 0.9475401163815906\n",
      "--------------------------------------------------\n",
      "Tuning Extra Trees...\n",
      "Best Params for Extra Trees: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 30, 'criterion': 'entropy', 'class_weight': 'balanced', 'bootstrap': True}\n",
      "Best CV R²: 0.9587550696526186\n",
      "--------------------------------------------------\n",
      "Tuning AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "45 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'algorithm' parameter of AdaBoostClassifier must be a str among {'SAMME'}. Got 'SAMME.R' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.85198378 0.88196085 0.87444895        nan 0.51842709 0.92318815\n",
      "        nan        nan        nan 0.95690354        nan 0.93819432\n",
      " 0.9025745         nan        nan 0.94755775 0.85387057        nan\n",
      " 0.41271381        nan]\n",
      "  warnings.warn(\n",
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for AdaBoost: {'n_estimators': 200, 'learning_rate': 0.01, 'algorithm': 'SAMME'}\n",
      "Best CV R²: 0.9569035443484394\n",
      "--------------------------------------------------\n",
      "Tuning Gradient Boosting...\n",
      "Best Params for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 7, 'learning_rate': 0.01}\n",
      "Best CV R²: 0.9418973725974255\n",
      "--------------------------------------------------\n",
      "Tuning XGBoost...\n",
      "Best Params for XGBoost: {'subsample': 1.0, 'reg_lambda': 1.5, 'reg_alpha': 0.1, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "Best CV R²: 0.95316522659143\n",
      "--------------------------------------------------\n",
      "Tuning CatBoost...\n",
      "Best Params for CatBoost: {'rsm': 0.8, 'random_strength': 2, 'min_data_in_leaf': 50, 'max_bin': 128, 'learning_rate': 0.03, 'l2_leaf_reg': 5, 'iterations': 500, 'grow_policy': 'SymmetricTree', 'depth': 12, 'bagging_temperature': 0}\n",
      "Best CV R²: 0.9662669723152882\n",
      "--------------------------------------------------\n",
      "Tuning Naive Bayes (Gaussian)...\n",
      "Best Params for Naive Bayes (Gaussian): {'var_smoothing': 1e-09}\n",
      "Best CV R²: 0.9249338740962794\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Insurance Claim Prediction Project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=20. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "best_models={}\n",
    "for name, model in classification_models.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=20,              # number of combinations to try\n",
    "        cv=5,                   # 5-fold cross-validation\n",
    "        scoring=\"recall\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    best_models[name] = search.best_estimator_\n",
    "    print(f\"Best Params for {name}: {search.best_params_}\")\n",
    "    print(f\"Best CV Recall Score: {search.best_score_}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b1578",
   "metadata": {},
   "source": [
    "## Out of all these models CatBosst Performs the best with the recall score -  0.9662669723152882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34122ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params={'rsm': 0.8, \n",
    "             'random_strength': 2, \n",
    "             'min_data_in_leaf': 50, \n",
    "             'max_bin': 128, \n",
    "             'learning_rate': 0.03, \n",
    "             'l2_leaf_reg': 5, \n",
    "             'iterations': 500, \n",
    "             'grow_policy': 'SymmetricTree', \n",
    "             'depth': 12, \n",
    "             'bagging_temperature': 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42794837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score 0.825\n",
      "Roc_Auc_Score 0.8284090909090909\n"
     ]
    }
   ],
   "source": [
    "best_model = CatBoostClassifier(**best_params,verbose=False,early_stopping_rounds=50,class_weights=[1, 5])\n",
    "best_model.fit(X_train,y_train)\n",
    "y_test_pred=best_model.predict(X_test)\n",
    "print(f'Recall Score {recall_score(y_test,y_test_pred)}')\n",
    "print(f'Roc_Auc_Score {roc_auc_score(y_test,y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "407fbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_file_path=os.path.join(os.getcwd(),'model.pkl')\n",
    "with open(model_file_path,'wb') as file_obj:\n",
    "    pickle.dump(best_models,file_obj)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
